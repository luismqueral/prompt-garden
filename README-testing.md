# Prompt Garden Testing Infrastructure

This testing infrastructure helps evaluate how well the Prompt Garden application scales with increasing amounts of data. It generates realistic test data and provides tools for measuring performance across various scenarios.

## Files Created

1. **test-data-generator.js**: Node.js script that generates test prompts of various types and adds them to your Google Sheet
2. **test-automation.sh**: Shell script for automating test execution with various options
3. **performance-metrics.md**: Template for recording performance metrics
4. **testing-execution-guide.md**: Step-by-step guide for executing the tests
5. **prompt-garden-testing-strategy.md**: Comprehensive testing strategy and implementation plan

## Key Features

- **Realistic Data Generation**: Creates diverse prompt types that mimic real user content
- **Batch Processing**: Efficiently adds large volumes of data using batched requests
- **Edge Case Testing**: Tests for extreme scenarios like very long prompts or many tags
- **Long Content Testing**: Systematically tests performance with increasing content lengths
- **Clean Up Functionality**: Easily remove test data when finished
- **Test Data Flagging**: Automatically marks test data to safely preserve real user content
- **Structured Metrics Collection**: Templates for consistent performance measurement
- **Flexible Testing Scale**: Test with 10, 100, 500, or 1000+ prompts

## Quick Start

1. Install dependencies:
   ```bash
   npm install google-auth-library googleapis uuid @faker-js/faker dotenv
   ```

2. Make the automation script executable:
   ```bash
   chmod +x test-automation.sh
   ```

3. Initialize the database (if not already done):
   ```bash
   ./test-automation.sh --init
   ```

4. Run a small test:
   ```bash
   ./test-automation.sh --small
   ```

5. For more options:
   ```bash
   ./test-automation.sh --help
   ```

## Testing Workflow

1. **Initialize**: Set up Google Sheets database
2. **Small Test**: Verify functionality with 10 prompts
3. **Scale Up**: Test with progressively larger data volumes
4. **Edge Cases**: Test special scenarios
5. **Long Content**: Test performance with varying content lengths
6. **Measure**: Record performance metrics
7. **Analyze**: Identify bottlenecks
8. **Optimize**: Implement improvements

## Available Options

| Command | Description |
|---------|-------------|
| `--init` | Guide through database initialization |
| `--small` | Run small test (10 prompts) |
| `--medium` | Run medium test (100 prompts) |
| `--large` | Run large test (500 prompts) |
| `--xl` | Run extra large test (1000 prompts) |
| `--edge-cases` | Generate edge case data |
| `--long-prompts` | Generate prompts with increasing lengths (5-100 paragraphs) |
| `--cleanup` | Remove test data only (preserves real user data) |
| `--cleanup-all` | Remove all data (use with caution) |
| `--full` | Run complete test cycle |
| `--help` | Show help information |

## Long Content Testing

The testing infrastructure includes specialized support for testing application performance with extremely long content:

1. **Variable Length Prompts**:
   - Generates a set of prompts with 5, 10, 20, 30, 50, 75, and 100 paragraphs
   - Each prompt is tagged with its paragraph count (e.g., "paragraphs-50")
   - Allows systematic measurement of performance degradation as content length increases

2. **Metrics Captured**:
   - Detail view render time
   - Scroll performance (FPS)
   - Edit operation latency
   - Memory consumption

3. **Usage**:
   ```bash
   ./test-automation.sh --long-prompts
   ```

4. **Analysis**:
   - The performance metrics template includes a dedicated section for recording long content metrics
   - A special breakdown table for the 50-paragraph test case helps pinpoint specific performance issues

## Test Data Management

The testing infrastructure includes a sophisticated mechanism for keeping track of test data:

1. **Test Data Flagging**:
   - A `test_data` column is automatically added to your Prompts sheet
   - All prompts generated by the testing scripts are marked as `TRUE`
   - Existing user data is marked as `FALSE`
   - This allows selective cleanup of test data without affecting real content

2. **Cleanup Options**:
   - `--cleanup`: Removes only flagged test data, preserving real user content
   - `--cleanup-all`: Removes all data (use with caution)

3. **Database Integrity**:
   - Tag counts are automatically recalculated after cleanup
   - Database structure is maintained

## Customization

To modify the test data generation:

1. **Add new prompt types**: Edit the `promptTypes` array in `generatePrompt()` function
2. **Change categories/tags**: Modify the `CATEGORIES` and `TAGS` arrays
3. **Add edge cases**: Extend the `generateEdgeCasePrompts()` function
4. **Modify long content tests**: Adjust the `lengths` array in `generateLongPromptsTest()` function

## Performance Optimization Opportunities

After running tests, you might consider these optimization strategies:

1. **Pagination**: For handling large datasets
2. **Virtual Scrolling**: To optimize rendering of many prompts
3. **Client-side Caching**: To reduce API calls
4. **Tag Usage Optimization**: Streamline tag filtering
5. **Search Optimization**: Improve search algorithm
6. **Lazy Loading**: For long content, consider rendering only visible portions
7. **Content Virtualization**: Replace off-screen content with placeholder elements
8. **Image and Asset Optimization**: Reduce load times

## Troubleshooting

See the "Troubleshooting" section in `testing-execution-guide.md` for common issues and solutions.

## Next Steps

After gathering performance data, refer to the "Next Steps After Testing" section in `testing-execution-guide.md` for guidance on analyzing results and implementing optimizations. 