# Prompt Garden Testing Execution Guide

This document provides step-by-step instructions for executing the performance testing plan for the Prompt Garden application.

## Prerequisites

1. ✅ Environment is set up with required dependencies:
   - Node.js 18+ and npm
   - Google Cloud Platform account with Google Sheets API enabled
   - Google Sheet and service account configured

2. ✅ Required files are created:
   - `test-data-generator.js` - Script for generating test data
   - `test-automation.sh` - Shell script for automating test execution
   - `performance-metrics.md` - Template for recording performance metrics

3. ✅ Dependencies are installed:
   ```bash
   npm install google-auth-library googleapis uuid @faker-js/faker dotenv
   ```

## Execution Steps

### Step 1: Initialize the Database

1. Start the development server:
   ```bash
   npm run dev
   ```

2. Navigate to the admin interface:
   ```
   http://localhost:3000/admin
   ```

3. Click the "Initialize Database" button to set up the necessary sheets.

4. Alternatively, use the automation script:
   ```bash
   ./test-automation.sh --init
   ```

### Step 2: Run a Small Test (10 Prompts)

1. Run the small test to verify functionality:
   ```bash
   ./test-automation.sh --small
   ```

2. Check the Google Sheet to confirm data was added correctly:
   - Verify prompts are created with appropriate content
   - Verify tags are tracked correctly
   - Note the `test_data` column contains `TRUE` for all generated prompts

3. Record baseline performance metrics in `performance-metrics.md`

### Step 3: Scale Up Testing

1. Clean up previous test data while preserving any real user data:
   ```bash
   ./test-automation.sh --cleanup
   ```

2. Run medium test (100 prompts):
   ```bash
   ./test-automation.sh --medium
   ```

3. Record performance metrics for medium data volume

4. Run large test (500 prompts):
   ```bash
   ./test-automation.sh --large
   ```

5. Record performance metrics for large data volume

6. Run extra large test (1000 prompts) if system can handle it:
   ```bash
   ./test-automation.sh --xl
   ```

7. Record performance metrics for extra large data volume

### Step 4: Test Edge Cases

1. Generate edge case prompts:
   ```bash
   ./test-automation.sh --edge-cases
   ```

2. Test and record performance for specific edge cases:
   - Very long prompt content
   - Prompts with many tags
   - Prompts with special characters
   - Prompts with minimal content

### Step 5: Test Long Content Performance

1. Generate a set of prompts with systematically increasing content lengths:
   ```bash
   ./test-automation.sh --long-prompts
   ```

2. This will create a series of prompts with 5, 10, 20, 30, 50, 75, and 100 paragraphs of content.

3. Each prompt is tagged with:
   - `long` - Identifies it as a long content test
   - `performance-test` - Marks it as a performance test
   - `paragraphs-X` - Indicates the exact paragraph count (e.g., `paragraphs-50`)

4. Test application performance with these prompts by:
   - Loading each prompt's detail view
   - Measuring render time for different content lengths
   - Testing edit functionality with very long content
   - Observing UI performance degradation as content length increases

5. Record your findings in the Performance Metrics template under the "Long Content Performance" section.

### Step 6: Full Test Cycle (Optional)

For a complete automated test cycle:

```bash
./test-automation.sh --full
```

This will:
1. Clean up existing test data (preserving real user data)
2. Run a small test (10 prompts)
3. Run a medium test (100 prompts)
4. Run a large test (500 prompts)
5. Generate edge cases

## Data Cleanup Options

The testing infrastructure includes two cleanup methods:

1. **Test Data Only Cleanup** (Recommended):
   ```bash
   ./test-automation.sh --cleanup
   ```
   This removes only data flagged as test data (`test_data=TRUE`), preserving any real user-created content.

2. **Complete Cleanup**:
   ```bash
   ./test-automation.sh --cleanup-all
   ```
   This removes ALL data from the sheets, including any real user content. Use with caution!

### How Test Data Flagging Works

- All prompts generated by the test script include a `test_data` column set to `TRUE`
- User-created prompts are automatically marked as `FALSE` in this column
- This allows selective cleanup of test data only
- When first run, the script automatically adds this column to your existing database

## Performance Metrics Collection

For each test scenario, record the following metrics in `performance-metrics.md`:

1. **Load Time Metrics**:
   - Time to First Byte (TTFB)
   - Time to Interactive (TTI)
   - First Paint
   - Memory Usage

2. **Filter Performance**:
   - Filter Time (from click to results displayed)
   - Render Time
   - Memory Usage

3. **Search Performance**:
   - Search Time (from query input to results displayed)
   - Render Time
   - Memory Usage

4. **Long Content Performance**:
   - Record metrics for each paragraph count (5, 10, 20, 30, 50, 75, 100)
   - Detail View Load Time
   - Scroll Performance (FPS)
   - Edit Operation Time
   - Memory Consumption

## Troubleshooting

### Common Issues

1. **Google Sheets API authorization errors**:
   - Verify the service account email is correct
   - Verify the service account has edit access to the Google Sheet
   - Verify the private key is correctly formatted with newlines as `\n`

2. **Script execution permission issues**:
   ```bash
   chmod +x test-automation.sh
   ```

3. **Database initialization issues**:
   - Check browser developer console for errors
   - Verify Google Sheets API is enabled in Google Cloud Console

4. **Missing test_data column**:
   - The script will automatically add this column when first run
   - If you encounter errors, check your sheet's structure
   - Ensure your prompts sheet has columns A-H with H being "test_data"

## Documentation

Keep detailed notes on:
1. Performance bottlenecks identified
2. Optimization recommendations
3. Any unusual behavior or edge case failures

## Next Steps After Testing

1. Analyze the performance metrics to identify bottlenecks
2. Implement recommended optimizations
3. Re-run tests to verify improvements
4. Document findings and optimization results 